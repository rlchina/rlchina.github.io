--- 
title: 课程内容 
icon: fa-th 
order: 4 
---
<p style="text-align:justify; text-justify:inter-ideograph;color: black">

0、汪军: Openning and Introduction <br />
<br />
1、卢宗青: Value-based Reinforcement Learning <br />
<b>- Introduction to Reinforcement Learning </b><br />
<b>    &emsp; - About RL </b><br />
<b>    &emsp; - RL problem </b><br />
<b>    &emsp; - Inside an RL agent </b><br />
<b>    &emsp; - Markov Decision Processes </b><br />
<b>- Value-based Methods </b><br />
<b>    &emsp; - Dynamic Programming </b><br />
<b>    &emsp; - Monte Carlo </b><br />
<b>    &emsp; - TD Learning </b><br />
<b>    &emsp; - Off-policy Learning </b><br />
<b>    &emsp; - DQN and its variants </b><br />
<br />
2、汪军: Policy-based RL and RL Theory <br />
<b>- Policy based approaches </b><br />
<b>    &emsp; - Policy gradient theorem </b><br />
<b>    &emsp; - REINFORCE algorithm </b><br />
<b>    &emsp; - Natural policy gradient </b><br />
<b>- PAC Learning theory </b><br />
<b>    &emsp; - Definition and concepts </b><br />
<b>    &emsp; - Concentration inequalities </b><br />
<b>    &emsp; - Uniform convergence </b><br />
<b>- RL theory </b><br />
<b>    &emsp; - Approximate Dynamic Programming </b><br />
<b>    &emsp; - Approximate Value Iteractions </b><br />
<b>    &emsp; - Approximate Policy Iterations </b><br />
<b>    &emsp; - Theoretical bounds and sample complexity analysis </b><br />
<br />
3、Haitham: Optimisation in Learning <br />
<b>- Motivation, Functions & Solution Types </b><br />
<b>    &emsp; - Applications of optimisation in Machine Learning </b><br />
<b>    &emsp; - Convex vs Non-Convex Optimisation Techniques </b><br />
<b>    &emsp; - Non-Convex Optimisation Solution Types </b><br />
<b>- Brief Survey of Optimisation Methods – Merits & Demerits </b><br />
<b>    &emsp; - Zero-Order Techniques </b><br />
<b>    &emsp; - First-Order Techniques </b><br />
<b>    &emsp; - Second-Order Techniques </b><br />
<b>- ADAM: An Adaptive Solver </b><br />
<b>    &emsp; - Brief History of ADAM </b><br />
<b>    &emsp; - ADAM’s Description </b><br />
<b>- ADAM’s Convergence Proof: </b><br />
<b>    &emsp; - Proof Strategy </b><br />
<b>    &emsp; - Assumptions </b><br />
<b>    &emsp; - Loss Function Difference Bound and Stationary Point Convergence </b><br />
<br />
4、张伟楠: Model-based Reinforcement Learning <br />
<b>- Model-based RL concepts </b><br />
<b>    &emsp; - Blackbox & whitebox models </b><br />
<b>- Classic MBRL </b><br />
<b>    &emsp; - Q-planning </b><br />
<b>    &emsp; - Dyna-Q </b><br />
<b>- Blackbox MBRL </b><br />
<b>    &emsp; - Model Predictive Control </b><br />
<b>    &emsp; - Probabilistic Ensemble & Trajectory Sampling </b><br />
<b>    &emsp; - Stochastic Lower Bound Optimization </b><br />
<b>    &emsp; - Model-based Policy Optimization </b><br />
<b>    &emsp; - Bidirectional Model based Policy Optimization </b><br />
<b>- Whitebox MBRL </b><br />
<b>    &emsp; - Stochastic Value Gradient </b><br />
<b>    &emsp; - Model Augmented Actor Critic </b><br />
<br />
5、朱占星: Control as Inference  <br />
<b>- Basics of probabilistic graphical models (D-separation, variational inference.) </b><br />
<b>- Connection between reinforcement learning and probabilistic inference.  </b><br />
<b>- Soft Q-learning  </b><br />
<b>- Entropy-regularized policy gradient </b><br />
<br />
6、俞扬: Imitation Learning <br />
<b>- Preliminary </b><br />
<b>- Supervised Learning & Behavior Cloning </b><br />
<b>- Generative Adversarial Learning & GAIL </b><br />
<b>- Advanced Topics </b><br />
<b>- From Imitating Policies to Imitating Environments </b><br />
<br />
7、郝建业: Hierarchical Reinforcement Learning <br />
<b>- From Sparse to Dense </b><br />
<b>    &emsp;- Reward Learning/Shaping </b><br />
<b>    &emsp;- Temporal/spatial credit assignment (single-agent/multiagent settings) </b><br />
<b>    &emsp;- Task hierarchical decomposition (hierarchical RL) </b><br />
<br />
8、张海峰: Game Theory Basics  <br />
<b>- Motivation and Normal-form Game </b><br />
<b>- Extensive-form Game and Imperfect Information </b><br />
<b>- Bayesian Game and Incomplete Information </b><br />
<b>- Nash Equilibrium and Variants </b><br />
<b>- Theoretical Results of Nash Equilibrium </b><br />
<b>- Repeated Game and Learning Methods </b><br />
<b>- Alternate Solution Concepts and Evolutionary Game Theory </b><br />
<br />
9、安波: Multi-agent Systems  <br />
<b>- History and Current Status </b><br />
<b>- Multi-agent Coordination and Negotiation </b><br />
<b>- Multi-agent Planning and Teamwork </b><br />
<b>- Distributed Constraint Optimization </b><br />
<b>- Multi-agent Organizational Design </b><br />
<b>- Game Theory and Mechanism Design </b><br />
<b>- Game Theory for Security </b><br />
<br />
10、张崇洁: Deep Multi-agent Learning<br />
<b>- Multi-Agent MDP and Dec-POMDP </b><br />
<b>- Centralized Training and Decentralized Execution and Individual-Global Maximization Principle </b><br />
<b>- Value-Based Deep MARL Methods </b><br />
<b>- Multi-Agent Policy Gradient Methods </b><br />
<b>- Theoretical Analysis  </b><br />
<br />
11、杨耀东: Advances in Multi-agent Learning <br />
<b>- Extensive-form games and its basics </b><br />
<b>- Ficiticious play </b><br />
<b>- Generalised weakened Ficiticious play </b><br />
<b>- Policy Space Response Oracle (PSRO) </b><br />
<b>- Alpha-Rank </b><br />
<b>- PSRO-Nash </b><br />
<b>- PSRO-Rechtified Nash </b><br />
<b>- PSRO-AlphaRank </b><br />
<br />
12、徐任远: Mean-field Games and Controls <br />
<b>- Mean-field game models  </b><br />
<b>    &emsp; - Examples  </b><br />
<b>    &emsp; - Approximation of non-cooperative games in the large population regime  </b><br />
<b>    &emsp; - Equilibrium concept  </b><br />
<b>- Existence and uniqueness of the equilibrium  </b><br />
<b>- Q-learning based algorithm  </b><br />
<b>    &emsp; - Stabilizing and smoothing techniques  </b><br />
<b>    &emsp; - Convergence and sample complexity analysis  </b><br />
<b>- General algorithms for learning mean-fifield games  </b><br />
<b>    &emsp; - Value-based algorithms (light touch)  </b><br />
<b>    &emsp; - Policy-based algorithms (light touch) </b><br />
<b>- Mean-field control models   </b><br />
<b>    &emsp; - Examples  </b><br />
<b>    &emsp; - Approximation of cooperative games in the large population regime  </b><br />
<b>    &emsp; - Social optimality condition  </b><br />
<b>- Dynamic programming principle on the probability measure space  </b><br />
<b>- Q-learning based algorithm  </b><br />
<b>    &emsp; - Kernel regression  </b><br />
<b>    &emsp; - Approximated Bellman update  </b><br />
<b>    &emsp; - Convergence and sample complexity analysis </b><br />
13、全体导师: Panel Discussion <br />
<br />
</p>